{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scrapping Day 2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract feed details from RSS in Python\n",
    "RSS = Rich Site Summary\n",
    "\n",
    "RSS means Rich Site Summary and uses standard web formats to publish information that changes frequently like blog posts, news, audio, video, etc. RSS documents often know as feed which consists of text, and metadata, like time and authorâ€™s name.\n",
    "\n",
    "# Getting RSS feed\n",
    "\n",
    "Use the feedparser.parse() function for creating a feed object which contains parsed blog. It takes the URL of the blog feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the modules\n",
    "import feedparser\n",
    "\n",
    "# fetch the feed with the url\n",
    "feed_url = 'https://vaibhavkumar.hashnode.dev/rss.xml'\n",
    "\n",
    "blog_feed = feedparser.parse(feed_url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting details from the blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduction to Stacks and Queues\n",
      "https://vaibhavkumar.hashnode.dev/introduction-to-stacks-and-queues\n",
      "vaibhav kumar\n",
      "Sun, 11 Jul 2021 12:33:53 GMT\n",
      "[]\n",
      "['vaibhav kumar']\n"
     ]
    }
   ],
   "source": [
    "#importing the modules\n",
    "import feedparser\n",
    "\n",
    "# fetch the feed with the URL\n",
    "feed_url = 'https://vaibhavkumar.hashnode.dev/rss.xml'\n",
    "\n",
    "blog_feed = feedparser.parse(feed_url)\n",
    "\n",
    "# returns link and number of blogs (entries) in the site\n",
    "blog_link = blog_feed.feed.link\n",
    "num_entries = len(blog_feed.entries)\n",
    "\n",
    "# details of individual blog accessed by the attribute name\n",
    "print(blog_feed.entries[0].title)\n",
    "print(blog_feed.entries[0].link)\n",
    "print(blog_feed.entries[0].author)\n",
    "print(blog_feed.entries[0].published)\n",
    "\n",
    "# getting list of tags and authors\n",
    "tags = []\n",
    "authors = []\n",
    "\n",
    "if hasattr(blog_feed.entries[0], 'tags'):\n",
    "    tags = [tag.term for tag in blog_feed.entries[0].tags]\n",
    "if hasattr(blog_feed.entries[0], 'authors'):\n",
    "    authors = [author.name for author in blog_feed.entries[0].authors]\n",
    "\n",
    "print(tags)\n",
    "print(authors)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the above code to write a function that takes the link of RSS feed and returns the details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Blog title\": \"Vaibhav's Blog\",\n",
      "  \"Blog link\": \"https://vaibhavkumar.hashnode.dev\",\n",
      "  \"entries\": [\n",
      "    {\n",
      "      \"title\": \"Introduction to Stacks and Queues\",\n",
      "      \"link\": \"https://vaibhavkumar.hashnode.dev/introduction-to-stacks-and-queues\",\n",
      "      \"author\": \"vaibhav kumar\",\n",
      "      \"published\": \"Sun, 11 Jul 2021 12:33:53 GMT\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"A new way of Scraping the Web: Requests-HTML\",\n",
      "      \"link\": \"https://vaibhavkumar.hashnode.dev/a-new-way-of-scraping-the-web-requests-html\",\n",
      "      \"author\": \"vaibhav kumar\",\n",
      "      \"published\": \"Fri, 08 Jan 2021 18:31:51 GMT\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Introduction to Image Classification using CNNs in PyTorch\",\n",
      "      \"link\": \"https://vaibhavkumar.hashnode.dev/introduction-to-image-classification-using-cnns-in-pytorch\",\n",
      "      \"author\": \"vaibhav kumar\",\n",
      "      \"published\": \"Thu, 31 Dec 2020 14:32:04 GMT\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Feed Forward Networks using PyTorch\",\n",
      "      \"link\": \"https://vaibhavkumar.hashnode.dev/feed-forward-networks-using-pytorch\",\n",
      "      \"author\": \"vaibhav kumar\",\n",
      "      \"published\": \"Tue, 29 Dec 2020 17:47:49 GMT\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Ridge Regression using Neural Network\",\n",
      "      \"link\": \"https://vaibhavkumar.hashnode.dev/ridge-regression-using-neural-network\",\n",
      "      \"author\": \"vaibhav kumar\",\n",
      "      \"published\": \"Tue, 29 Dec 2020 14:23:50 GMT\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Working with Hashnode RSS feed in Python\",\n",
      "      \"link\": \"https://vaibhavkumar.hashnode.dev/working-with-hashnode-rss-feed-in-python\",\n",
      "      \"author\": \"vaibhav kumar\",\n",
      "      \"published\": \"Mon, 28 Dec 2020 15:59:28 GMT\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Concatenating PDF files using Python\",\n",
      "      \"link\": \"https://vaibhavkumar.hashnode.dev/concatenating-pdf-files-using-python\",\n",
      "      \"author\": \"vaibhav kumar\",\n",
      "      \"published\": \"Tue, 22 Dec 2020 11:18:13 GMT\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Building a Command Line File Downloader in Python\",\n",
      "      \"link\": \"https://vaibhavkumar.hashnode.dev/building-a-command-line-file-downloader-in-python\",\n",
      "      \"author\": \"vaibhav kumar\",\n",
      "      \"published\": \"Tue, 22 Dec 2020 08:00:37 GMT\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Neural Style Tranfer\",\n",
      "      \"link\": \"https://vaibhavkumar.hashnode.dev/neural-style-tranfer\",\n",
      "      \"author\": \"vaibhav kumar\",\n",
      "      \"published\": \"Wed, 16 Dec 2020 17:45:14 GMT\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Introduction to Generative-Adversarial Networks(GANs) using PyTorch\",\n",
      "      \"link\": \"https://vaibhavkumar.hashnode.dev/introduction-to-generative-adversarial-networksgans-using-pytorch\",\n",
      "      \"author\": \"vaibhav kumar\",\n",
      "      \"published\": \"Tue, 15 Dec 2020 14:56:17 GMT\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def Rss_feed(rss=None):\n",
    "    #taking the link of the RSS feed\n",
    "    if rss is not None:\n",
    "        import feedparser\n",
    "        #parsing the blog feed\n",
    "        blog_feed = blog_feed = feedparser.parse(rss)\n",
    "\n",
    "        #getting list of the blog entries\n",
    "        entries = blog_feed.entries\n",
    "\n",
    "        #holding the details in the dictionary form\n",
    "        details = {\"Blog title\":blog_feed.feed.title,\n",
    "                   \"Blog link\":blog_feed.feed.link}\n",
    "        \n",
    "        #making a list for the iteration\n",
    "        p_list=[]\n",
    "\n",
    "        #iterating over each one of the details\n",
    "        for entry in entries:\n",
    "            temp=dict()\n",
    "\n",
    "            #using try and except statement to avoid the error during the execution\n",
    "            try:\n",
    "                temp[\"title\"]=entry.title\n",
    "                temp[\"link\"]=entry.link\n",
    "                temp[\"author\"]=entry.author\n",
    "                temp[\"published\"]=entry.published\n",
    "                temp[\"authors\"]=[author.name for author in entry.author]\n",
    "                temp[\"summary\"]=entry.summary\n",
    "            except:\n",
    "                pass\n",
    "            p_list.append(temp)\n",
    "\n",
    "        details[\"entries\"]=p_list\n",
    "        return details\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    import json\n",
    "    feed_url=\"https://vaibhavkumar.hashnode.dev/rss.xml\"\n",
    "    datas = Rss_feed(rss=feed_url)\n",
    "\n",
    "    if datas:\n",
    "        print(json.dumps(datas,indent=2))\n",
    "    else:\n",
    "        print(\"None\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get tag name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name property is provided by Beautiful Soup which is a web scraping framework for Python. Web scraping is the process of extracting data from the website using automated tools to make the process faster. Name object corresponds to the name of an XML or HTML tag in the original document.\n",
    "\n",
    "Syntax:\n",
    "\n",
    "tag.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the object with an XML string\n",
    "soup = BeautifulSoup(\"\"\"<root><name_of_tag>the first strong tag</name_of_tag></root>\"\"\", \"lxml\")\n",
    "\n",
    "# Get the tag\n",
    "tag = soup.find('name_of_tag')\n",
    "\n",
    "# Get the tag name\n",
    "name = tag.name\n",
    "\n",
    "# Print the output\n",
    "print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the object with a HTML page\n",
    "soup = BeautifulSoup('''\n",
    "\t<html>\n",
    "\t\t<h2> Heading 1 </h2>\n",
    "\t\t<h1> Heading 2 </h1>\n",
    "\t</html>\n",
    "\t''', \"lxml\")\n",
    "\n",
    "# Get the whole h2 tag\n",
    "tag = soup.h2\n",
    "\n",
    "# Get the name of the tag\n",
    "name = tag.name\n",
    "\n",
    "# Print the output\n",
    "print(name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents list â€“ Python Beautifulsoup\n",
    "The contents list is provided by Beautiful Soup which is a web scraping framework for Python. Web scraping is the process of extracting data from the website using automated tools to make the process faster. The content is a list that contains the tagâ€™s children.\n",
    "\n",
    "Syntax: \n",
    "\n",
    " tag.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<b> Hello world </b>, <body></body>]\n"
     ]
    }
   ],
   "source": [
    "# Import Beautiful Soup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Create the document\n",
    "doc = \"<body><b> Hello world </b><body>\"\n",
    "\n",
    "# Initialize the object with the document\n",
    "soup = BeautifulSoup(doc, \"html.parser\")\n",
    "\n",
    "# Get the whole content from the body tag\n",
    "contents = soup.body.contents\n",
    "\n",
    "# Print the contents\n",
    "print(contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Import Beautiful Soup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Create the document\n",
    "doc = \"<body><b> Hello world </b><body>\"\n",
    "\n",
    "# Initialize the object with the document\n",
    "soup = BeautifulSoup(doc, \"html.parser\")\n",
    "\n",
    "# Get the whole content from the body tag\n",
    "contents = soup.body.contents\n",
    "\n",
    "# Print the type of contents\n",
    "print(type(contents))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping the data of Flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â‚¹364\n",
      "â‚¹27,995\n",
      "â‚¹2,397\n",
      "â‚¹4,497\n",
      "â‚¹19,995\n",
      "â‚¹12,495\n",
      "â‚¹1,701\n",
      "â‚¹4,272\n",
      "â‚¹22,995\n",
      "â‚¹12,495\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BS\n",
    "\n",
    "link = 'https://www.flipkart.com/watches/pr?sid=r18'\n",
    "\n",
    "flipkart_link = requests.get(link)\n",
    "\n",
    "soup = BS(flipkart_link.text, 'html.parser')\n",
    "\n",
    "elements = soup.find_all(\"div\", class_=\"_1AtVbE\")\n",
    "\n",
    "for element in elements:\n",
    "    if element:\n",
    "        a = element.find(\"div\", class_=\"_13oc-S _1t9ceu\")\n",
    "        if a:\n",
    "            b = a.find(\"div\", class_=\"_1xHGtK _373qXS\")\n",
    "            if b:\n",
    "                c = b.find(\"div\", class_=\"_2B099V\")\n",
    "                if c:\n",
    "                    d = c.find(\"div\", class_=\"_25b18c\")\n",
    "                    if d:\n",
    "                        e = d.find(\"div\", class_=\"_30jeq3\")\n",
    "                        if e:\n",
    "                            print(e.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
